{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c573ea-ab39-424b-b9b3-b9e5bee3a06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061c86f1-1f35-4e3a-ae25-3f0383b13767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "document = data['English Text (Google Translate)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f16c12-72ec-4d11-954a-c4596aff216a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         God bless him for being a great human being.\n",
       "1    I love the humanity of our astronaut, a man wi...\n",
       "2    Two great men making history. Thank you for al...\n",
       "3    Nayib and Frank, two great examples of humilit...\n",
       "4    Colombia lacks a precedent with this one, I am...\n",
       "Name: English Text (Google Translate), dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c279229c-7e24-4e9d-bc10-b9dc6e096adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/smit/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9669f3-c929-4b23-9b2a-cb0f7b42805c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Lemmatize and remove stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f27201e-42da-4615-a5c9-fd32e5d04c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a4c6c43-a17a-4a49-b296-4d8fcec34619",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                [bless, great, human]\n",
       "1    [love, human, astronaut, high, level, academ, ...\n",
       "2    [great, make, histori, thank, allow, feel, pro...\n",
       "3    [nayib, frank, great, exampl, humil, wisdom, s...\n",
       "4    [colombia, lack, preced, faith, follow, presid...\n",
       "5                           [great, proud, salvadoran]\n",
       "6              [doubt, admir, salvadoran, feel, proud]\n",
       "7    [success, salvadoran, charact, help, conquest,...\n",
       "8                           [salvadoran, pride, greet]\n",
       "9    [astronaut, doctor, colonel, frank, rubio, pro...\n",
       "Name: English Text (Google Translate), dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = document.map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657cd4ed-a6a2-44e9-82f1-cab4d9e6c2fd",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6258205-b94a-4193-8cb7-447647ca28ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b31097a9-369f-4866-b849-524af4396811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47743df2-89ef-4ad4-a49e-1867f701592b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bless\n",
      "1 great\n",
      "2 astronaut\n",
      "3 love\n",
      "4 pride\n",
      "5 countri\n",
      "6 proud\n",
      "7 thank\n",
      "8 exampl\n",
      "9 frank\n",
      "10 nayib\n",
      "11 salvador\n",
      "12 wisdom\n",
      "13 bukel\n",
      "14 presid\n",
      "15 salvadoran\n",
      "16 admir\n",
      "17 good\n",
      "18 peopl\n",
      "19 greet\n",
      "20 rubio\n",
      "21 congratul\n",
      "22 live\n",
      "23 beauti\n",
      "24 continu\n",
      "25 like\n",
      "26 famili\n",
      "27 best\n",
      "28 world\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k,v in dictionary.iteritems():\n",
    "    count+=1\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a31948-be22-41ed-92d8-28ddf9490cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bcd525-c1de-4f2f-aa95-d9f708c9f3cd",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0185274d-c0b8-48f6-9436-5f928bfeba58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0dafa5f-12d5-4612-ae4c-f789a8ea98e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dcd3b6e-7112-4c5e-a599-faf7c894de0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.45524926832276413), (1, 0.8903640287498075)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_tfidf:\n",
    "    print(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce4dd0-5c94-40fb-a323-e96627a11425",
   "metadata": {},
   "source": [
    "#### Running LDA using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2b16616-eaec-4cd1-9d37-1b17dde4ecd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05fc0df7-0af8-4ed5-92bc-77c4d690e863",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.129*\"love\" + 0.125*\"bless\" + 0.104*\"thank\" + 0.079*\"presid\" + 0.066*\"bukel\" + 0.062*\"countri\" + 0.061*\"continu\" + 0.056*\"famili\" + 0.053*\"salvador\" + 0.039*\"peopl\"\n",
      "Topic: 1 \n",
      "Words: 0.150*\"peopl\" + 0.149*\"salvadoran\" + 0.141*\"great\" + 0.108*\"pride\" + 0.081*\"bless\" + 0.045*\"exampl\" + 0.044*\"congratul\" + 0.041*\"countri\" + 0.037*\"world\" + 0.032*\"salvador\"\n",
      "Topic: 2 \n",
      "Words: 0.161*\"presid\" + 0.147*\"bless\" + 0.130*\"salvador\" + 0.072*\"countri\" + 0.066*\"admir\" + 0.058*\"continu\" + 0.048*\"bukel\" + 0.040*\"beauti\" + 0.039*\"wisdom\" + 0.037*\"like\"\n",
      "Topic: 3 \n",
      "Words: 0.179*\"good\" + 0.135*\"thank\" + 0.127*\"bless\" + 0.083*\"world\" + 0.080*\"peopl\" + 0.063*\"salvadoran\" + 0.053*\"presid\" + 0.050*\"live\" + 0.040*\"love\" + 0.040*\"bukel\"\n",
      "Topic: 4 \n",
      "Words: 0.117*\"countri\" + 0.115*\"presid\" + 0.096*\"bukel\" + 0.092*\"bless\" + 0.079*\"like\" + 0.074*\"nayib\" + 0.057*\"thank\" + 0.045*\"great\" + 0.041*\"proud\" + 0.040*\"salvadoran\"\n",
      "Topic: 5 \n",
      "Words: 0.317*\"presid\" + 0.145*\"bukel\" + 0.093*\"bless\" + 0.055*\"famili\" + 0.055*\"world\" + 0.048*\"nayib\" + 0.042*\"countri\" + 0.037*\"best\" + 0.032*\"thank\" + 0.024*\"great\"\n",
      "Topic: 6 \n",
      "Words: 0.254*\"bless\" + 0.091*\"like\" + 0.074*\"congratul\" + 0.071*\"proud\" + 0.070*\"continu\" + 0.066*\"presid\" + 0.050*\"salvadoran\" + 0.047*\"good\" + 0.044*\"salvador\" + 0.042*\"greet\"\n",
      "Topic: 7 \n",
      "Words: 0.180*\"bless\" + 0.111*\"great\" + 0.098*\"presid\" + 0.087*\"salvadoran\" + 0.070*\"greet\" + 0.052*\"salvador\" + 0.051*\"beauti\" + 0.048*\"countri\" + 0.044*\"thank\" + 0.044*\"admir\"\n",
      "Topic: 8 \n",
      "Words: 0.282*\"best\" + 0.156*\"presid\" + 0.141*\"greet\" + 0.083*\"bless\" + 0.065*\"world\" + 0.030*\"nayib\" + 0.029*\"salvadoran\" + 0.026*\"admir\" + 0.024*\"countri\" + 0.024*\"bukel\"\n",
      "Topic: 9 \n",
      "Words: 0.099*\"salvadoran\" + 0.096*\"rubio\" + 0.079*\"frank\" + 0.078*\"countri\" + 0.075*\"great\" + 0.073*\"bless\" + 0.058*\"astronaut\" + 0.041*\"exampl\" + 0.039*\"world\" + 0.037*\"salvador\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105b120-f8c3-4898-b5a0-65d619b1a788",
   "metadata": {},
   "source": [
    "#### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ed579e8-4350-48ab-b854-65b9be1ade1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.185*\"beauti\" + 0.146*\"good\" + 0.140*\"famili\" + 0.065*\"presid\" + 0.055*\"bless\" + 0.051*\"like\" + 0.048*\"salvador\" + 0.037*\"continu\" + 0.032*\"bukel\" + 0.029*\"world\"\n",
      "Topic: 1 Word: 0.158*\"world\" + 0.106*\"continu\" + 0.095*\"peopl\" + 0.082*\"thank\" + 0.077*\"salvador\" + 0.062*\"bless\" + 0.058*\"countri\" + 0.041*\"presid\" + 0.034*\"good\" + 0.034*\"pride\"\n",
      "Topic: 2 Word: 0.172*\"great\" + 0.138*\"peopl\" + 0.129*\"congratul\" + 0.099*\"exampl\" + 0.089*\"bless\" + 0.064*\"salvador\" + 0.041*\"salvadoran\" + 0.034*\"presid\" + 0.034*\"wisdom\" + 0.031*\"rubio\"\n",
      "Topic: 3 Word: 0.193*\"like\" + 0.153*\"proud\" + 0.073*\"great\" + 0.062*\"salvadoran\" + 0.054*\"bless\" + 0.051*\"countri\" + 0.048*\"presid\" + 0.042*\"famili\" + 0.041*\"pride\" + 0.040*\"peopl\"\n",
      "Topic: 4 Word: 0.158*\"bukel\" + 0.128*\"love\" + 0.095*\"salvador\" + 0.090*\"presid\" + 0.084*\"bless\" + 0.078*\"nayib\" + 0.062*\"thank\" + 0.036*\"like\" + 0.030*\"world\" + 0.028*\"continu\"\n",
      "Topic: 5 Word: 0.227*\"bless\" + 0.175*\"countri\" + 0.102*\"presid\" + 0.055*\"bukel\" + 0.055*\"love\" + 0.051*\"salvador\" + 0.049*\"congratul\" + 0.045*\"salvadoran\" + 0.043*\"peopl\" + 0.038*\"best\"\n",
      "Topic: 6 Word: 0.121*\"salvadoran\" + 0.093*\"pride\" + 0.089*\"greet\" + 0.076*\"great\" + 0.060*\"frank\" + 0.060*\"rubio\" + 0.058*\"astronaut\" + 0.049*\"bukel\" + 0.047*\"bless\" + 0.044*\"presid\"\n",
      "Topic: 7 Word: 0.229*\"thank\" + 0.228*\"best\" + 0.073*\"bless\" + 0.073*\"presid\" + 0.064*\"good\" + 0.040*\"wisdom\" + 0.036*\"continu\" + 0.034*\"countri\" + 0.034*\"salvadoran\" + 0.029*\"proud\"\n",
      "Topic: 8 Word: 0.233*\"presid\" + 0.118*\"countri\" + 0.099*\"live\" + 0.088*\"world\" + 0.062*\"best\" + 0.061*\"like\" + 0.042*\"salvador\" + 0.036*\"bless\" + 0.034*\"bukel\" + 0.032*\"wisdom\"\n",
      "Topic: 9 Word: 0.248*\"admir\" + 0.082*\"salvador\" + 0.064*\"salvadoran\" + 0.053*\"bless\" + 0.052*\"congratul\" + 0.048*\"presid\" + 0.048*\"continu\" + 0.037*\"bukel\" + 0.037*\"like\" + 0.033*\"great\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
