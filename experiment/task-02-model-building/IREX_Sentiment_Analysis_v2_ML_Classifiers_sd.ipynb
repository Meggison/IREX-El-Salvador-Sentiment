{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqZU-gK0IeSO",
        "outputId": "4c7b8d3b-f1c8-4324-f11b-0ba6d29f19e9"
      },
      "source": [
        "!pip install spacy\n",
        "\n",
        "!pip install mlflow dagshubm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'DagsHubAuth' from 'dagshub.auth' (C:\\Users\\DhalS\\AppData\\Roaming\\Python\\Python311\\site-packages\\dagshub\\auth\\__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdagshub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DAGsHubLogger\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdagshub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DagsHubAuth\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDAGSHUB_USER\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOUR_DAGSHUB_USERNAME\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'DagsHubAuth' from 'dagshub.auth' (C:\\Users\\DhalS\\AppData\\Roaming\\Python\\Python311\\site-packages\\dagshub\\auth\\__init__.py)"
          ]
        }
      ],
      "source": [
        "from dagshub import DAGsHubLogger\n",
        "from dagshub.auth import DagsHubAuth\n",
        "import os\n",
        "\n",
        "os.environ['DAGSHUB_USER'] = 'YOUR_DAGSHUB_USERNAME'\n",
        "os.environ['DAGSHUB_TOKEN'] = 'YOUR_DAGSHUB_TOKEN'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Client created. Use the name of the repo <span style=\"font-weight: bold\">(</span>IREX-El-Salvador-Sentiment<span style=\"font-weight: bold\">)</span> as the name of the bucket\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Client created. Use the name of the repo \u001b[1m(\u001b[0mIREX-El-Salvador-Sentiment\u001b[1m)\u001b[0m as the name of the bucket\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<botocore.client.S3 at 0x16ce383cc50>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dagshub import get_repo_bucket_client\n",
        "s3 = get_repo_bucket_client(\"Omdena/IREX-El-Salvador-Sentiment\")\n",
        "s3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "s3.download_file(\n",
        "    Bucket=\"IREX-El-Salvador-Sentiment\",  # name of the repo\n",
        "    Key=\"train_df_2100_manual_final.csv\",  #  remote path from where to download the file\n",
        "    Filename=\"train_df_2100_manual_final.csv\",  # local path where to download the file\n",
        ")\n",
        "\n",
        "s3.download_file(\n",
        "    Bucket=\"IREX-El-Salvador-Sentiment\",  # name of the repo\n",
        "    Key=\"validation_df_450_manual_final.csv\",  #  remote path from where to download the file\n",
        "    Filename=\"validation_df_450_manual_final.csv\",  # local path where to download the file\n",
        ")\n",
        "\n",
        "s3.download_file(\n",
        "    Bucket=\"IREX-El-Salvador-Sentiment\",  # name of the repo\n",
        "    Key=\"test_df_450_manual_final.csv\",  #  remote path from where to download the file\n",
        "    Filename=\"test_df_450_manual_final.csv\",  # local path where to download the file\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "KDYAkYaF0bu7",
        "outputId": "8e1a073d-3ea6-4186-a390-f0054013cbe3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(r'C:\\Users\\DhalS\\OneDrive - Kantar\\Desktop\\Omdena\\IREX-El-Salvador-Sentiment\\experiment\\task-02-model-building\\train_df_2100_manual_final.csv')\n",
        "df_val = pd.read_csv(r'C:\\Users\\DhalS\\OneDrive - Kantar\\Desktop\\Omdena\\IREX-El-Salvador-Sentiment\\experiment\\task-02-model-building\\validation_df_450_manual_final.csv')\n",
        "df_test = pd.read_csv(r'C:\\Users\\DhalS\\OneDrive - Kantar\\Desktop\\Omdena\\IREX-El-Salvador-Sentiment\\experiment\\task-02-model-building\\test_df_450_manual_final.csv')\n",
        "\n",
        "\n",
        "\n",
        "total_df = pd.concat([df_train, df_val, df_test], axis=0)\n",
        "total_df.shape\n",
        "\n",
        "df_train.rename(columns={'manual labeling': 'manual_labeling'}, inplace=True)\n",
        "df_val.rename(columns={'manual labeling': 'manual_labeling'}, inplace=True)\n",
        "df_test.rename(columns={'manual labeling': 'manual_labeling'}, inplace=True)\n",
        "\n",
        "total_df.rename(columns={'manual labeling': 'manual_labeling'}, inplace=True)\n",
        "\n",
        "\n",
        "df_train['manual_labeling_new'] = df_train['manual_labeling'].replace({-1: 0, 0: 1, 1: 2})\n",
        "df_test['manual_labeling_new'] = df_test['manual_labeling'].replace({-1: 0, 0: 1, 1: 2})\n",
        "df_val['manual_labeling_new'] = df_val['manual_labeling'].replace({-1: 0, 0: 1, 1: 2})\n",
        "\n",
        "total_df['manual_labeling_new'] = total_df['manual_labeling'].replace({-1: 0, 0: 1, 1: 2})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "axUBk9C01vaK",
        "outputId": "c16aa7cd-d1ed-4a9f-d766-bb610f099ebc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>comment_url</th>\n",
              "      <th>comment_createdAt</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>comment_viewCount</th>\n",
              "      <th>comment_lang</th>\n",
              "      <th>comment_author_createdAt</th>\n",
              "      <th>comment_location</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>...</th>\n",
              "      <th>post_followers</th>\n",
              "      <th>post_verified</th>\n",
              "      <th>post_text</th>\n",
              "      <th>isActor_self__decalred__location__El_Salvador</th>\n",
              "      <th>post_processed_text</th>\n",
              "      <th>actor_affiliation</th>\n",
              "      <th>llama3_output</th>\n",
              "      <th>llama3_reason</th>\n",
              "      <th>manual_labeling</th>\n",
              "      <th>manual_labeling_new</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>532</td>\n",
              "      <td>1209</td>\n",
              "      <td>https://x.com/dany_u07/status/1773424640678838281</td>\n",
              "      <td>1.711652e+09</td>\n",
              "      <td>1.773420e+18</td>\n",
              "      <td>67.0</td>\n",
              "      <td>es</td>\n",
              "      <td>08/02/2018</td>\n",
              "      <td>Houston, TX</td>\n",
              "      <td>@nayibbukele Uff lancha nueva üòÅ</td>\n",
              "      <td>...</td>\n",
              "      <td>6211020.0</td>\n",
              "      <td>True</td>\n",
              "      <td>La embarcaci√≥n que transportaba la droga ya es...</td>\n",
              "      <td>True</td>\n",
              "      <td>la embarcaci√≥n que transportaba la droga ya es...</td>\n",
              "      <td>Government</td>\n",
              "      <td>0.0</td>\n",
              "      <td>The tweet is neutral, simply stating the arriv...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1968</td>\n",
              "      <td>6219</td>\n",
              "      <td>https://x.com/marquinan40/status/1761554116961...</td>\n",
              "      <td>1.708822e+09</td>\n",
              "      <td>1.761550e+18</td>\n",
              "      <td>99.0</td>\n",
              "      <td>es</td>\n",
              "      <td>09/08/2022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@Vi11atoro @PNCSV Es lo menos q podemos hacer ...</td>\n",
              "      <td>...</td>\n",
              "      <td>66429.0</td>\n",
              "      <td>True</td>\n",
              "      <td>Agradecemos a la poblaci√≥n la confianza en rea...</td>\n",
              "      <td>False</td>\n",
              "      <td>agradecemos a la poblaci√≥n la confianza en rea...</td>\n",
              "      <td>Government</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The tweet expresses gratitude and appreciation...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2293</td>\n",
              "      <td>7359</td>\n",
              "      <td>https://x.com/UnidadTps/status/176163107190775...</td>\n",
              "      <td>1.708841e+09</td>\n",
              "      <td>1.761630e+18</td>\n",
              "      <td>13.0</td>\n",
              "      <td>es</td>\n",
              "      <td>22/01/2016</td>\n",
              "      <td>Dallas, TX</td>\n",
              "      <td>@MarceloLarin1 Todavia lloras porque no te die...</td>\n",
              "      <td>...</td>\n",
              "      <td>64221.0</td>\n",
              "      <td>True</td>\n",
              "      <td>Hasta hoy la mujer m√°s coherente que he visto ...</td>\n",
              "      <td>False</td>\n",
              "      <td>hasta hoy la mujer m√°s coherente que he visto ...</td>\n",
              "      <td>Media</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>The tweet expresses criticism and disappointme...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2249</td>\n",
              "      <td>7197</td>\n",
              "      <td>https://x.com/lachele78/status/176466206779601...</td>\n",
              "      <td>1.709563e+09</td>\n",
              "      <td>1.764660e+18</td>\n",
              "      <td>7957.0</td>\n",
              "      <td>es</td>\n",
              "      <td>05/09/2020</td>\n",
              "      <td>El Salvador</td>\n",
              "      <td>@MarceloLarin1 @eybonluis Lonque el no entiend...</td>\n",
              "      <td>...</td>\n",
              "      <td>64221.0</td>\n",
              "      <td>True</td>\n",
              "      <td>Nayib les quito el fodes a sus alcaldes, no ll...</td>\n",
              "      <td>False</td>\n",
              "      <td>nayib les quito el fodes a sus alcaldes no lle...</td>\n",
              "      <td>Media</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>The tweet expresses a negative sentiment as it...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2525</td>\n",
              "      <td>8144</td>\n",
              "      <td>https://x.com/BlancaEstela506/status/136669961...</td>\n",
              "      <td>1.614682e+09</td>\n",
              "      <td>1.366700e+18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>es</td>\n",
              "      <td>06/06/2019</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@FranAlabi @nayibbukele Excelente!¬°!  muy boni...</td>\n",
              "      <td>...</td>\n",
              "      <td>410702.0</td>\n",
              "      <td>True</td>\n",
              "      <td>√Åreas dignas para la poblaci√≥n y el personal d...</td>\n",
              "      <td>True</td>\n",
              "      <td>√°reas dignas para la poblaci√≥n y el personal d...</td>\n",
              "      <td>Government</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The tweet expresses a positive sentiment by us...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 37 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0.1  Unnamed: 0  \\\n",
              "0           532        1209   \n",
              "1          1968        6219   \n",
              "2          2293        7359   \n",
              "3          2249        7197   \n",
              "4          2525        8144   \n",
              "\n",
              "                                         comment_url  comment_createdAt  \\\n",
              "0  https://x.com/dany_u07/status/1773424640678838281       1.711652e+09   \n",
              "1  https://x.com/marquinan40/status/1761554116961...       1.708822e+09   \n",
              "2  https://x.com/UnidadTps/status/176163107190775...       1.708841e+09   \n",
              "3  https://x.com/lachele78/status/176466206779601...       1.709563e+09   \n",
              "4  https://x.com/BlancaEstela506/status/136669961...       1.614682e+09   \n",
              "\n",
              "     comment_id  comment_viewCount comment_lang comment_author_createdAt  \\\n",
              "0  1.773420e+18               67.0           es               08/02/2018   \n",
              "1  1.761550e+18               99.0           es               09/08/2022   \n",
              "2  1.761630e+18               13.0           es               22/01/2016   \n",
              "3  1.764660e+18             7957.0           es               05/09/2020   \n",
              "4  1.366700e+18                NaN           es               06/06/2019   \n",
              "\n",
              "  comment_location                                       comment_text  ...  \\\n",
              "0      Houston, TX                    @nayibbukele Uff lancha nueva üòÅ  ...   \n",
              "1              NaN  @Vi11atoro @PNCSV Es lo menos q podemos hacer ...  ...   \n",
              "2       Dallas, TX  @MarceloLarin1 Todavia lloras porque no te die...  ...   \n",
              "3      El Salvador  @MarceloLarin1 @eybonluis Lonque el no entiend...  ...   \n",
              "4              NaN  @FranAlabi @nayibbukele Excelente!¬°!  muy boni...  ...   \n",
              "\n",
              "   post_followers post_verified  \\\n",
              "0       6211020.0          True   \n",
              "1         66429.0          True   \n",
              "2         64221.0          True   \n",
              "3         64221.0          True   \n",
              "4        410702.0          True   \n",
              "\n",
              "                                           post_text  \\\n",
              "0  La embarcaci√≥n que transportaba la droga ya es...   \n",
              "1  Agradecemos a la poblaci√≥n la confianza en rea...   \n",
              "2  Hasta hoy la mujer m√°s coherente que he visto ...   \n",
              "3  Nayib les quito el fodes a sus alcaldes, no ll...   \n",
              "4  √Åreas dignas para la poblaci√≥n y el personal d...   \n",
              "\n",
              "   isActor_self__decalred__location__El_Salvador  \\\n",
              "0                                           True   \n",
              "1                                          False   \n",
              "2                                          False   \n",
              "3                                          False   \n",
              "4                                           True   \n",
              "\n",
              "                                 post_processed_text  actor_affiliation  \\\n",
              "0  la embarcaci√≥n que transportaba la droga ya es...         Government   \n",
              "1  agradecemos a la poblaci√≥n la confianza en rea...         Government   \n",
              "2  hasta hoy la mujer m√°s coherente que he visto ...              Media   \n",
              "3  nayib les quito el fodes a sus alcaldes no lle...              Media   \n",
              "4  √°reas dignas para la poblaci√≥n y el personal d...         Government   \n",
              "\n",
              "   llama3_output                                      llama3_reason  \\\n",
              "0            0.0  The tweet is neutral, simply stating the arriv...   \n",
              "1            1.0  The tweet expresses gratitude and appreciation...   \n",
              "2           -1.0  The tweet expresses criticism and disappointme...   \n",
              "3           -1.0  The tweet expresses a negative sentiment as it...   \n",
              "4            1.0  The tweet expresses a positive sentiment by us...   \n",
              "\n",
              "   manual_labeling  manual_labeling_new  \n",
              "0                0                    1  \n",
              "1                1                    2  \n",
              "2               -1                    0  \n",
              "3               -1                    0  \n",
              "4                1                    2  \n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mCDY4XpIWhg",
        "outputId": "401df15e-1a8d-4aa4-89f5-e03a5d423fbf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\DhalS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\DhalS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import string\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "import dagshub\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "spacy.cli.download(\"es_core_news_sm\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZvcRIgc88N0T"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load the spaCy model for Spanish\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Initialize the Snowball stemmer for Spanish\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "# Define a list of stopwords\n",
        "stop_words = set(nltk.corpus.stopwords.words('spanish'))\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters (keeping numbers) and convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë√º√ú0-9]', ' ', text)\n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Reconstruct the text\n",
        "    processed_text = ' '.join(filtered_tokens)\n",
        "    return processed_text\n",
        "\n",
        "# Function to apply lemmatization and stemming to text\n",
        "def lemmatize_and_stem(text):\n",
        "    # Lemmatize using spaCy\n",
        "    doc = nlp(text)\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "    # Stem using NLTK SnowballStemmer\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "\n",
        "    # Reconstruct the text\n",
        "    processed_text = ' '.join(stemmed_tokens)\n",
        "    return processed_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NNGHgq328Qni"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing, lemmatization, and stemming to the 'Text' column\n",
        "total_df['processed_text_final'] = total_df['post_processed_text'].apply(preprocess_text).apply(lemmatize_and_stem)\n",
        "df_train['processed_text_final'] = df_train['post_processed_text'].apply(preprocess_text).apply(lemmatize_and_stem)\n",
        "df_val['processed_text_final'] = df_val['post_processed_text'].apply(preprocess_text).apply(lemmatize_and_stem)\n",
        "\n",
        "df_test['processed_text_final'] = df_test['post_processed_text'].apply(preprocess_text).apply(lemmatize_and_stem)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DALTCbKsOvr1",
        "outputId": "fd1bf231-5748-401b-9d46-f26a41200dd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Unnamed: 0.1                                          0\n",
              "Unnamed: 0                                            0\n",
              "comment_url                                           0\n",
              "comment_createdAt                                     0\n",
              "comment_id                                            0\n",
              "comment_viewCount                                   896\n",
              "comment_lang                                          0\n",
              "comment_author_createdAt                              0\n",
              "comment_location                                   1723\n",
              "comment_text                                          0\n",
              "isComment_self__decalred__location__El_Salvador       0\n",
              "comment_processed_text                                0\n",
              "post_url                                              0\n",
              "post_actor_createdAt                                  0\n",
              "post_id                                               0\n",
              "post_isReply                                          0\n",
              "post_inReplyToId                                   2940\n",
              "post_isRetweet                                        0\n",
              "post_isQuote                                          0\n",
              "post_viewCount                                      893\n",
              "post_retweetCount                                     0\n",
              "post_likeCount                                        0\n",
              "post_replyCount                                       0\n",
              "post_lang                                             0\n",
              "actor_location                                      390\n",
              "post_name                                             0\n",
              "post_description                                      0\n",
              "post_followers                                        0\n",
              "post_verified                                         0\n",
              "post_text                                             0\n",
              "isActor_self__decalred__location__El_Salvador         0\n",
              "post_processed_text                                   0\n",
              "actor_affiliation                                   176\n",
              "llama3_output                                         0\n",
              "llama3_reason                                         0\n",
              "manual_labeling                                       0\n",
              "manual_labeling_new                                   0\n",
              "processed_text_final                                  0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "oZVTum8TGLJ2",
        "outputId": "3b8e017c-a993-4e9b-8df6-f49e4afe97a8"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_word_cloud(dataframe, column_name, width=800, height=400, background_color='white'):\n",
        "    \"\"\"\n",
        "    Generate a word cloud from preprocessed text in a DataFrame column.\n",
        "\n",
        "    Args:\n",
        "    - dataframe: The DataFrame containing the text data.\n",
        "    - column_name: The name of the column containing the preprocessed text.\n",
        "    - width: Width of the word cloud image.\n",
        "    - height: Height of the word cloud image.\n",
        "    - background_color: Background color of the word cloud image.\n",
        "\n",
        "    Returns:\n",
        "    - None. Displays the word cloud image.\n",
        "    \"\"\"\n",
        "    # Combine all preprocessed text into a single string\n",
        "    all_text = ' '.join(dataframe[column_name])\n",
        "\n",
        "    # Generate a word cloud image\n",
        "    wordcloud = WordCloud(width=width, height=height, background_color=background_color).generate(all_text)\n",
        "\n",
        "    # Display the word cloud image\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "generate_word_cloud(total_df, 'processed_text_final')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "V2M98dyaOSR3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import mlflow\n",
        "from dagshub import dagshub_logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "jlzGO7cqOZuo",
        "outputId": "c9eec7b5-9d3d-4aa5-f0e0-48d4fc128351"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"Omdena/IREX-El-Salvador-Sentiment\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"Omdena/IREX-El-Salvador-Sentiment\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository Omdena/IREX-El-Salvador-Sentiment initialized!\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Repository Omdena/IREX-El-Salvador-Sentiment initialized!\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# Initialize DagsHub for MLflow tracking\n",
        "dagshub.init(\"IREX-El-Salvador-Sentiment\", \"Omdena\", mlflow=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBo95NzYGLJ2",
        "outputId": "50a9dda4-e579-4c82-bc3b-4538cc471e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the TF-IDF matrix for training data: (2100, 1000)\n",
            "Shape of the TF-IDF matrix for test data: (450, 1000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and test sets\n",
        "\n",
        "X_train = df_train['processed_text_final']\n",
        "y_train = df_train['manual_labeling_new']\n",
        "\n",
        "X_val = df_val['processed_text_final']\n",
        "y_val = df_val['manual_labeling_new']\n",
        "\n",
        "X_test = df_test['processed_text_final']\n",
        "y_test = df_test['manual_labeling_new']\n",
        "\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# Fit-transform the training data and transform the test data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Check the shape of the TF-IDF matrix\n",
        "print(\"Shape of the TF-IDF matrix for training data:\", X_train_tfidf.shape)\n",
        "print(\"Shape of the TF-IDF matrix for test data:\", X_test_tfidf.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1jr1ScrAMnYk"
      },
      "outputs": [],
      "source": [
        "# Define a list of classifiers\n",
        "classifiers = {\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Support Vector Machine': SVC(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'XGBoost': XGBClassifier(),\n",
        "    'Naive Bayes': MultinomialNB()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eloLKl4AOMxU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TyK2qkl2OM0O"
      },
      "outputs": [],
      "source": [
        "# Define a function to train and evaluate the classifier\n",
        "def train_and_evaluate_classifier(clf, clf_name, X_train, y_train, X_test, y_test):\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the classifier on training data\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "\n",
        "    # Evaluate the classifier on test data\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "    f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
        "    precision = precision_score(y_test, y_pred_test, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred_test, average='weighted')\n",
        "\n",
        "    # Log the experiment with MLflow and DagsHub\n",
        "    with mlflow.start_run(run_name=clf_name):\n",
        "        # Log parameters\n",
        "        mlflow.log_params(clf.get_params())\n",
        "        mlflow.log_param(\"Classifier\", clf_name)\n",
        "\n",
        "        # Log metrics\n",
        "        mlflow.log_metric(\"Training Accuracy\", train_accuracy)\n",
        "        mlflow.log_metric(\"accuracy\", test_accuracy)\n",
        "        mlflow.log_metric(\"f1_score\", f1)\n",
        "        mlflow.log_metric(\"precision_score\", precision)\n",
        "        mlflow.log_metric(\"recall_score\", recall)\n",
        "\n",
        "        # Log the model\n",
        "        mlflow.sklearn.log_model(clf, f'{clf_name}_model')\n",
        "\n",
        "        # Add tags\n",
        "        mlflow.set_tag(\"estimator_name\", clf_name)\n",
        "        mlflow.set_tag(\"transformer\", \"TfidfVectorizer\")\n",
        "\n",
        "    return pd.DataFrame({'Classifier': [clf_name], 'Training Accuracy': [train_accuracy], 'Test Accuracy': [test_accuracy]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUoU24TK53_9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "J0ufW0U_OM3D",
        "outputId": "a18e3abc-e4a8-4257-9669-bfffc3a18f0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DhalS\\AppData\\Local\\Temp\\ipykernel_19108\\1652227561.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, results], ignore_index=True)\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "c:\\Program Files\\Python311\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Classifier</th>\n",
              "      <th>Training Accuracy</th>\n",
              "      <th>Test Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.547143</td>\n",
              "      <td>0.524444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.613810</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.615238</td>\n",
              "      <td>0.593333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>0.615238</td>\n",
              "      <td>0.593333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>0.615238</td>\n",
              "      <td>0.593333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.615238</td>\n",
              "      <td>0.591111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.593333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Classifier  Training Accuracy  Test Accuracy\n",
              "0                     KNN           0.547143       0.524444\n",
              "1     Logistic Regression           0.613810       0.600000\n",
              "2           Random Forest           0.615238       0.593333\n",
              "3  Support Vector Machine           0.615238       0.593333\n",
              "4           Decision Tree           0.615238       0.593333\n",
              "5                 XGBoost           0.615238       0.591111\n",
              "6             Naive Bayes           0.607143       0.593333"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize an empty DataFrame to store the results\n",
        "results_df = pd.DataFrame(columns=['Classifier', 'Training Accuracy', 'Test Accuracy'])\n",
        "\n",
        "# Iterate over each classifier\n",
        "for clf_name, clf in classifiers.items():\n",
        "    # Train and evaluate the classifier\n",
        "    results = train_and_evaluate_classifier(clf, clf_name, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
        "\n",
        "    # Append the results to the DataFrame\n",
        "    results_df = pd.concat([results_df, results], ignore_index=True)\n",
        "\n",
        "results_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
